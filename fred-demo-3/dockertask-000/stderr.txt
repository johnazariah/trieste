CNTK 1.7.2+ (master e4a8cf, Oct 21 2016 16:04:01) on localhost at 2016/11/10 22:44:54

/cntk/build-mkl/cpu/release/bin/cntk  configFile=/cntk/Examples/Image/Classification/ConvNet/ConvNet_MNIST.cntk  rootDir=.  dataDir=/cntk/Examples/Image/DataSets/MNIST
-------------------------------------------------------------------
Build info: 

		Built time: Oct 21 2016 16:04:01
		Last modified date: Fri Oct 21 15:51:10 2016
		Build type: release
		Build target: CPU-only
		With 1bit-SGD: yes
		Math lib: mkl
		Build Branch: master
		Build SHA1: e4a8cf41630cee04ed8939ed0f58d5cbaee66837
		Built by  on 4fbc279817bb
		Build Path: /cntk
-------------------------------------------------------------------

Configuration After Processing and Variable Resolution:

configparameters: ConvNet_MNIST.cntk:command=trainNetwork:testNetwork
configparameters: ConvNet_MNIST.cntk:dataDir=/cntk/Examples/Image/DataSets/MNIST
configparameters: ConvNet_MNIST.cntk:deviceId=auto
configparameters: ConvNet_MNIST.cntk:modelPath=./Output/Models/ConvNet_MNIST
configparameters: ConvNet_MNIST.cntk:outputDir=./Output
configparameters: ConvNet_MNIST.cntk:precision=float
configparameters: ConvNet_MNIST.cntk:rootDir=.
configparameters: ConvNet_MNIST.cntk:testNetwork={
    action = test
minibatchSize = 1024    
    reader = {
        readerType = "CNTKTextFormatReader"
        file = "/cntk/Examples/Image/DataSets/MNIST/Test-28x28_cntk_text.txt"
        input = {
            features = { dim = 784 ; format = "dense" }
            labels =   { dim = 10  ; format = "dense" }
        }
    }
}

configparameters: ConvNet_MNIST.cntk:traceLevel=1
configparameters: ConvNet_MNIST.cntk:trainNetwork={
    action = "train"
    BrainScriptNetworkBuilder = {
imageShape = 28:28:1                        
labelDim = 10                               
        featScale = 1/256
        Scale{f} = x => Constant(f) .* x
        model = Sequential (
            Scale {featScale} :
            ConvolutionalLayer {32, (5:5), pad = true} : ReLU : 
            MaxPoolingLayer    {(3:3), stride=(2:2)} :
            ConvolutionalLayer {48, (3:3), pad = false} : ReLU : 
            MaxPoolingLayer    {(3:3), stride=(2:2)} :
            ConvolutionalLayer {64, (3:3), pad = false} : ReLU : 
            DenseLayer         {96} : Dropout : ReLU :  
            LinearLayer        {labelDim}
        )
        features = Input {imageShape}
        labels = Input {labelDim}
        ol = model (features)
        ce   = CrossEntropyWithSoftmax (labels, ol)
        errs = ClassificationError (labels, ol)
        featureNodes    = (features)
        labelNodes      = (labels)
        criterionNodes  = (ce)
        evaluationNodes = (errs)
        outputNodes     = (ol)
    }
    SGD = {
        epochSize = 60000
        minibatchSize = 64
        maxEpochs = 40
        learningRatesPerSample = 0.001*10:0.0005*10:0.0001
		dropoutRate = 0.5
        momentumAsTimeConstant = 0*5:1024
        numMBsToShowResult = 500
    }
    reader = {
        readerType = "CNTKTextFormatReader"
        file = "/cntk/Examples/Image/DataSets/MNIST/Train-28x28_cntk_text.txt"
        randomize = true
        keepDataInMemory = true
        input = {
            features = { dim = 784 ; format = "dense" }
            labels =   { dim = 10  ; format = "dense" }
        }
    }    
}

Commands: trainNetwork testNetwork
precision = "float"

##############################################################################
#                                                                            #
# trainNetwork command (train action)                                        #
#                                                                            #
##############################################################################


Creating virgin network.
Node '<placeholder>' (LearnableParameter operation): Initializating Parameter[10 x 0] as glorotUniform later when dimensions are fully known.
Node '<placeholder>' (LearnableParameter operation): Initializating Parameter[96 x 0] as glorotUniform later when dimensions are fully known.
Node '<placeholder>' (LearnableParameter operation): Initializating Parameter[3 x 3 x 0 x 64] as glorotUniform later when dimensions are fully known.
Node '<placeholder>' (LearnableParameter operation): Initializating Parameter[3 x 3 x 0 x 48] as glorotUniform later when dimensions are fully known.
Node '<placeholder>' (LearnableParameter operation): Initializating Parameter[5 x 5 x 0 x 32] as glorotUniform later when dimensions are fully known.

Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	errs = ClassificationError()
	ol = Plus()

Validating network. 33 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [10 x *]
Validating --> model.arrayOfFunctions[12].W = LearnableParameter() :  -> [10 x 0]
Validating --> model.arrayOfFunctions[9].arrayOfFunctions[0].W = LearnableParameter() :  -> [96 x 0]
Validating --> model.arrayOfFunctions[7].W = LearnableParameter() :  -> [3 x 3 x 0 x 64]
Validating --> model.arrayOfFunctions[4].W = LearnableParameter() :  -> [3 x 3 x 0 x 48]
Validating --> model.arrayOfFunctions[1].W = LearnableParameter() :  -> [5 x 5 x 0 x 32]
Validating --> ol.x._._.x._.x.x._.x.x._.x.ElementTimesArgs[0] = LearnableParameter() :  -> [1 x 1]
Validating --> features = InputValue() :  -> [28 x 28 x 1 x *]
Validating --> ol.x._._.x._.x.x._.x.x._.x = ElementTimes (ol.x._._.x._.x.x._.x.x._.x.ElementTimesArgs[0], features) : [1 x 1], [28 x 28 x 1 x *] -> [28 x 28 x 1 x *]
Node 'model.arrayOfFunctions[1].W' (LearnableParameter operation) operation: Tensor shape was inferred as [5 x 5 x 1 x 32].
Node 'model.arrayOfFunctions[1].W' (LearnableParameter operation): Initializing Parameter[5 x 5 x 1 x 32] <- glorotUniform(seed=5, init dims=[800 x 25], range=0.085280*1.000000, onCPU=true.
)Validating --> ol.x._._.x._.x.x._.x.x._.c = Convolution (model.arrayOfFunctions[1].W, ol.x._._.x._.x.x._.x.x._.x) : [5 x 5 x 1 x 32], [28 x 28 x 1 x *] -> [28 x 28 x 32 x *]
Validating --> model.arrayOfFunctions[1].b = LearnableParameter() :  -> [1 x 1 x 32]
Validating --> ol.x._._.x._.x.x._.x.x._.res.x = Plus (ol.x._._.x._.x.x._.x.x._.c, model.arrayOfFunctions[1].b) : [28 x 28 x 32 x *], [1 x 1 x 32] -> [28 x 28 x 32 x *]
Validating --> ol.x._._.x._.x.x._.x.x = RectifiedLinear (ol.x._._.x._.x.x._.x.x._.res.x) : [28 x 28 x 32 x *] -> [28 x 28 x 32 x *]
Validating --> ol.x._._.x._.x.x._.x = Pooling (ol.x._._.x._.x.x._.x.x) : [28 x 28 x 32 x *] -> [13 x 13 x 32 x *]
Node 'model.arrayOfFunctions[4].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 48].
Node 'model.arrayOfFunctions[4].W' (LearnableParameter operation): Initializing Parameter[3 x 3 x 32 x 48] <- glorotUniform(seed=4, init dims=[432 x 288], range=0.091287*1.000000, onCPU=true.
)Validating --> ol.x._._.x._.x.x._.c = Convolution (model.arrayOfFunctions[4].W, ol.x._._.x._.x.x._.x) : [3 x 3 x 32 x 48], [13 x 13 x 32 x *] -> [11 x 11 x 48 x *]
Validating --> model.arrayOfFunctions[4].b = LearnableParameter() :  -> [1 x 1 x 48]
Validating --> ol.x._._.x._.x.x._.res.x = Plus (ol.x._._.x._.x.x._.c, model.arrayOfFunctions[4].b) : [11 x 11 x 48 x *], [1 x 1 x 48] -> [11 x 11 x 48 x *]
Validating --> ol.x._._.x._.x.x = RectifiedLinear (ol.x._._.x._.x.x._.res.x) : [11 x 11 x 48 x *] -> [11 x 11 x 48 x *]
Validating --> ol.x._._.x._.x = Pooling (ol.x._._.x._.x.x) : [11 x 11 x 48 x *] -> [5 x 5 x 48 x *]
Node 'model.arrayOfFunctions[7].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 48 x 64].
Node 'model.arrayOfFunctions[7].W' (LearnableParameter operation): Initializing Parameter[3 x 3 x 48 x 64] <- glorotUniform(seed=3, init dims=[576 x 432], range=0.077152*1.000000, onCPU=true.
)Validating --> ol.x._._.x._.c = Convolution (model.arrayOfFunctions[7].W, ol.x._._.x._.x) : [3 x 3 x 48 x 64], [5 x 5 x 48 x *] -> [3 x 3 x 64 x *]
Validating --> model.arrayOfFunctions[7].b = LearnableParameter() :  -> [1 x 1 x 64]
Validating --> ol.x._._.x._.res.x = Plus (ol.x._._.x._.c, model.arrayOfFunctions[7].b) : [3 x 3 x 64 x *], [1 x 1 x 64] -> [3 x 3 x 64 x *]
Validating --> _ol.x._._.x = RectifiedLinear (ol.x._._.x._.res.x) : [3 x 3 x 64 x *] -> [3 x 3 x 64 x *]
Node 'model.arrayOfFunctions[9].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [96 x 3 x 3 x 64].
Node 'model.arrayOfFunctions[9].arrayOfFunctions[0].W' (LearnableParameter operation): Initializing Parameter[96 x 3 x 3 x 64] <- glorotUniform(seed=2, init dims=[96 x 576], range=0.094491*1.000000, onCPU=true.
)Validating --> ol.x._._.x.PlusArgs[0] = Times (model.arrayOfFunctions[9].arrayOfFunctions[0].W, _ol.x._._.x) : [96 x 3 x 3 x 64], [3 x 3 x 64 x *] -> [96 x *]
Validating --> model.arrayOfFunctions[9].arrayOfFunctions[0].b = LearnableParameter() :  -> [96]
Validating --> ol.x._._.x = Plus (ol.x._._.x.PlusArgs[0], model.arrayOfFunctions[9].arrayOfFunctions[0].b) : [96 x *], [96] -> [96 x *]
Validating --> ol.x._ = Dropout (ol.x._._.x) : [96 x *] -> [96 x *]
Validating --> ol.x = RectifiedLinear (ol.x._) : [96 x *] -> [96 x *]
Node 'model.arrayOfFunctions[12].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 96].
Node 'model.arrayOfFunctions[12].W' (LearnableParameter operation): Initializing Parameter[10 x 96] <- glorotUniform(seed=1, init dims=[10 x 96], range=0.237915*1.000000, onCPU=true.
)Validating --> ol.PlusArgs[0] = Times (model.arrayOfFunctions[12].W, ol.x) : [10 x 96], [96 x *] -> [10 x *]
Validating --> model.arrayOfFunctions[12].b = LearnableParameter() :  -> [10]
Validating --> ol = Plus (ol.PlusArgs[0], model.arrayOfFunctions[12].b) : [10 x *], [10] -> [10 x *]
Validating --> ce = CrossEntropyWithSoftmax (labels, ol) : [10 x *], [10 x *] -> [1]
Validating --> errs = ClassificationError (labels, ol) : [10 x *], [10 x *] -> [1]

Validating network. 20 nodes to process in pass 2.


Validating network, final pass.

ol.x._._.x._.x.x._.x.x._.c: using GEMM convolution engine for geometry: Input: 28 x 28 x 1, Output: 28 x 28 x 32, Kernel: 5 x 5 x 1, Map: 32, Stride: 1 x 1 x 1, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
ol.x._._.x._.x.x._.x: using GEMM convolution engine for geometry: Input: 28 x 28 x 32, Output: 13 x 13 x 32, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1, 1, 1), AutoPad: (0, 0, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
ol.x._._.x._.x.x._.c: using GEMM convolution engine for geometry: Input: 13 x 13 x 32, Output: 11 x 11 x 48, Kernel: 3 x 3 x 32, Map: 48, Stride: 1 x 1 x 32, Sharing: (1, 1, 1), AutoPad: (0, 0, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
ol.x._._.x._.x: using GEMM convolution engine for geometry: Input: 11 x 11 x 48, Output: 5 x 5 x 48, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1, 1, 1), AutoPad: (0, 0, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
ol.x._._.x._.c: using GEMM convolution engine for geometry: Input: 5 x 5 x 48, Output: 3 x 3 x 64, Kernel: 3 x 3 x 48, Map: 64, Stride: 1 x 1 x 48, Sharing: (1, 1, 1), AutoPad: (0, 0, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.



Post-processing network complete.


Model has 33 nodes. Using CPU.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 61 matrices, 36 are shared as 16, and 25 are not shared.

	{ model.arrayOfFunctions[1].W : [5 x 5 x 1 x 32] (gradient)
	  ol.x._._.x._.x.x._.x.x._.res.x : [28 x 28 x 32 x *] }
	{ ol.x._._.x._.x.x._.x.x : [28 x 28 x 32 x *]
	  ol.x._._.x._.x.x._.x.x._.c : [28 x 28 x 32 x *] (gradient) }
	{ ol.x._._.x._.x.x._.x : [13 x 13 x 32 x *]
	  ol.x._._.x._.x.x._.x.x._.res.x : [28 x 28 x 32 x *] (gradient) }
	{ model.arrayOfFunctions[1].b : [1 x 1 x 32] (gradient)
	  ol.x._._.x._.x.x._.x.x : [28 x 28 x 32 x *] (gradient) }
	{ model.arrayOfFunctions[4].W : [3 x 3 x 32 x 48] (gradient)
	  ol.x._._.x._.x.x._.res.x : [11 x 11 x 48 x *] }
	{ ol.x._._.x._.x.x : [11 x 11 x 48 x *]
	  ol.x._._.x._.x.x._.c : [11 x 11 x 48 x *] (gradient) }
	{ ol.x._._.x._.x : [5 x 5 x 48 x *]
	  ol.x._._.x._.x.x._.res.x : [11 x 11 x 48 x *] (gradient)
	  ol.x._._.x._.x.x._.x : [13 x 13 x 32 x *] (gradient) }
	{ model.arrayOfFunctions[4].b : [1 x 1 x 48] (gradient)
	  ol.x._._.x._.x.x : [11 x 11 x 48 x *] (gradient) }
	{ model.arrayOfFunctions[7].W : [3 x 3 x 48 x 64] (gradient)
	  ol.x._._.x._.res.x : [3 x 3 x 64 x *] }
	{ _ol.x._._.x : [3 x 3 x 64 x *]
	  ol.x._._.x._.c : [3 x 3 x 64 x *] (gradient) }
	{ ol.x._._.x.PlusArgs[0] : [96 x *]
	  ol.x._._.x._.res.x : [3 x 3 x 64 x *] (gradient)
	  ol.x._._.x._.x : [5 x 5 x 48 x *] (gradient) }
	{ model.arrayOfFunctions[9].arrayOfFunctions[0].W : [96 x 3 x 3 x 64] (gradient)
	  ol.x._._.x : [96 x *] }
	{ model.arrayOfFunctions[9].arrayOfFunctions[0].b : [96] (gradient)
	  ol.x._ : [96 x *] }
	{ _ol.x._._.x : [3 x 3 x 64 x *] (gradient)
	  model.arrayOfFunctions[7].b : [1 x 1 x 64] (gradient)
	  ol.x : [96 x *]
	  ol.x._._.x : [96 x *] (gradient) }
	{ ol.PlusArgs[0] : [10 x *]
	  ol.x._ : [96 x *] (gradient) }
	{ model.arrayOfFunctions[12].W : [10 x 96] (gradient)
	  ol : [10 x *] (gradient) }


Training 98778 parameters in 10 out of 10 parameter tensors and 28 nodes with gradient:

	Node 'model.arrayOfFunctions[12].W' (LearnableParameter operation) : [10 x 96]
	Node 'model.arrayOfFunctions[12].b' (LearnableParameter operation) : [10]
	Node 'model.arrayOfFunctions[1].W' (LearnableParameter operation) : [5 x 5 x 1 x 32]
	Node 'model.arrayOfFunctions[1].b' (LearnableParameter operation) : [1 x 1 x 32]
	Node 'model.arrayOfFunctions[4].W' (LearnableParameter operation) : [3 x 3 x 32 x 48]
	Node 'model.arrayOfFunctions[4].b' (LearnableParameter operation) : [1 x 1 x 48]
	Node 'model.arrayOfFunctions[7].W' (LearnableParameter operation) : [3 x 3 x 48 x 64]
	Node 'model.arrayOfFunctions[7].b' (LearnableParameter operation) : [1 x 1 x 64]
	Node 'model.arrayOfFunctions[9].arrayOfFunctions[0].W' (LearnableParameter operation) : [96 x 3 x 3 x 64]
	Node 'model.arrayOfFunctions[9].arrayOfFunctions[0].b' (LearnableParameter operation) : [96]

No PreCompute nodes found, or all already computed. Skipping pre-computation step.
Setting dropout rate to 0.5.

Starting Epoch 1: learning rate per sample = 0.001000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

Starting minibatch loop.
 Epoch[ 1 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.61894873 * 32000; errs = 19.831% * 32000; time = 57.2366s; samplesPerSecond = 559.1
Finished Epoch[ 1 of 40]: [Training] ce = 0.39874854 * 60000; errs = 12.613% * 60000; totalSamplesSeen = 60000; learningRatePerSample = 0.001; epochTime=105.49s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.1'

Starting Epoch 2: learning rate per sample = 0.001000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

Starting minibatch loop.
 Epoch[ 2 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.10342358 * 32000; errs = 3.087% * 32000; time = 56.5967s; samplesPerSecond = 565.4
Finished Epoch[ 2 of 40]: [Training] ce = 0.10046725 * 60000; errs = 2.952% * 60000; totalSamplesSeen = 120000; learningRatePerSample = 0.001; epochTime=106.233s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.2'

Starting Epoch 3: learning rate per sample = 0.001000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

Starting minibatch loop.
 Epoch[ 3 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.07692087 * 32000; errs = 2.234% * 32000; time = 54.5157s; samplesPerSecond = 587.0
Finished Epoch[ 3 of 40]: [Training] ce = 0.07418381 * 60000; errs = 2.107% * 60000; totalSamplesSeen = 180000; learningRatePerSample = 0.001; epochTime=101.869s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.3'

Starting Epoch 4: learning rate per sample = 0.001000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

Starting minibatch loop.
 Epoch[ 4 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.06031287 * 32000; errs = 1.719% * 32000; time = 55.8914s; samplesPerSecond = 572.5
Finished Epoch[ 4 of 40]: [Training] ce = 0.05890625 * 60000; errs = 1.743% * 60000; totalSamplesSeen = 240000; learningRatePerSample = 0.001; epochTime=103.657s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.4'

Starting Epoch 5: learning rate per sample = 0.001000  effective momentum = 0.000000  momentum as time constant = 0.0 samples

Starting minibatch loop.
 Epoch[ 5 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.05262003 * 32000; errs = 1.484% * 32000; time = 55.0485s; samplesPerSecond = 581.3
Finished Epoch[ 5 of 40]: [Training] ce = 0.05197273 * 60000; errs = 1.468% * 60000; totalSamplesSeen = 300000; learningRatePerSample = 0.001; epochTime=103.063s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.5'

Starting Epoch 6: learning rate per sample = 0.001000  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[ 6 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.04804892 * 32000; errs = 1.419% * 32000; time = 54.6687s; samplesPerSecond = 585.3
Finished Epoch[ 6 of 40]: [Training] ce = 0.04591748 * 60000; errs = 1.322% * 60000; totalSamplesSeen = 360000; learningRatePerSample = 0.001; epochTime=102.899s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.6'

Starting Epoch 7: learning rate per sample = 0.001000  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[ 7 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.04189948 * 32000; errs = 1.259% * 32000; time = 56.3088s; samplesPerSecond = 568.3
Finished Epoch[ 7 of 40]: [Training] ce = 0.04194075 * 60000; errs = 1.258% * 60000; totalSamplesSeen = 420000; learningRatePerSample = 0.001; epochTime=103.836s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.7'

Starting Epoch 8: learning rate per sample = 0.001000  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[ 8 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.03808862 * 32000; errs = 1.066% * 32000; time = 54.4199s; samplesPerSecond = 588.0
Finished Epoch[ 8 of 40]: [Training] ce = 0.03683650 * 60000; errs = 1.043% * 60000; totalSamplesSeen = 480000; learningRatePerSample = 0.001; epochTime=101.587s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.8'

Starting Epoch 9: learning rate per sample = 0.001000  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[ 9 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.03428347 * 32000; errs = 0.981% * 32000; time = 56.0373s; samplesPerSecond = 571.0
Finished Epoch[ 9 of 40]: [Training] ce = 0.03304268 * 60000; errs = 0.973% * 60000; totalSamplesSeen = 540000; learningRatePerSample = 0.001; epochTime=105.704s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.9'

Starting Epoch 10: learning rate per sample = 0.001000  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[10 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.03044171 * 32000; errs = 0.887% * 32000; time = 58.6861s; samplesPerSecond = 545.3
Finished Epoch[10 of 40]: [Training] ce = 0.02991526 * 60000; errs = 0.885% * 60000; totalSamplesSeen = 600000; learningRatePerSample = 0.001; epochTime=107.799s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.10'

Starting Epoch 11: learning rate per sample = 0.000500  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[11 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.02414504 * 32000; errs = 0.700% * 32000; time = 55.4669s; samplesPerSecond = 576.9
Finished Epoch[11 of 40]: [Training] ce = 0.02426157 * 60000; errs = 0.708% * 60000; totalSamplesSeen = 660000; learningRatePerSample = 0.00050000002; epochTime=103.832s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.11'

Starting Epoch 12: learning rate per sample = 0.000500  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[12 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.02111059 * 32000; errs = 0.600% * 32000; time = 56.5294s; samplesPerSecond = 566.1
Finished Epoch[12 of 40]: [Training] ce = 0.02194733 * 60000; errs = 0.645% * 60000; totalSamplesSeen = 720000; learningRatePerSample = 0.00050000002; epochTime=106.142s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.12'

Starting Epoch 13: learning rate per sample = 0.000500  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[13 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01833551 * 32000; errs = 0.516% * 32000; time = 55.2133s; samplesPerSecond = 579.6
Finished Epoch[13 of 40]: [Training] ce = 0.02032676 * 60000; errs = 0.575% * 60000; totalSamplesSeen = 780000; learningRatePerSample = 0.00050000002; epochTime=104.992s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.13'

Starting Epoch 14: learning rate per sample = 0.000500  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[14 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.02016940 * 32000; errs = 0.600% * 32000; time = 56.6058s; samplesPerSecond = 565.3
Finished Epoch[14 of 40]: [Training] ce = 0.02001485 * 60000; errs = 0.603% * 60000; totalSamplesSeen = 840000; learningRatePerSample = 0.00050000002; epochTime=106.769s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.14'

Starting Epoch 15: learning rate per sample = 0.000500  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[15 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01734187 * 32000; errs = 0.547% * 32000; time = 55.4695s; samplesPerSecond = 576.9
Finished Epoch[15 of 40]: [Training] ce = 0.01821629 * 60000; errs = 0.540% * 60000; totalSamplesSeen = 900000; learningRatePerSample = 0.00050000002; epochTime=104.251s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.15'

Starting Epoch 16: learning rate per sample = 0.000500  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[16 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01761166 * 32000; errs = 0.522% * 32000; time = 55.6421s; samplesPerSecond = 575.1
Finished Epoch[16 of 40]: [Training] ce = 0.01755251 * 60000; errs = 0.532% * 60000; totalSamplesSeen = 960000; learningRatePerSample = 0.00050000002; epochTime=103.191s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.16'

Starting Epoch 17: learning rate per sample = 0.000500  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[17 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01713373 * 32000; errs = 0.522% * 32000; time = 54.1820s; samplesPerSecond = 590.6
Finished Epoch[17 of 40]: [Training] ce = 0.01779559 * 60000; errs = 0.518% * 60000; totalSamplesSeen = 1020000; learningRatePerSample = 0.00050000002; epochTime=102.755s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.17'

Starting Epoch 18: learning rate per sample = 0.000500  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[18 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01528950 * 32000; errs = 0.428% * 32000; time = 53.5460s; samplesPerSecond = 597.6
Finished Epoch[18 of 40]: [Training] ce = 0.01650815 * 60000; errs = 0.493% * 60000; totalSamplesSeen = 1080000; learningRatePerSample = 0.00050000002; epochTime=101.707s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.18'

Starting Epoch 19: learning rate per sample = 0.000500  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[19 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01624757 * 32000; errs = 0.475% * 32000; time = 55.2469s; samplesPerSecond = 579.2
Finished Epoch[19 of 40]: [Training] ce = 0.01578847 * 60000; errs = 0.465% * 60000; totalSamplesSeen = 1140000; learningRatePerSample = 0.00050000002; epochTime=103.171s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.19'

Starting Epoch 20: learning rate per sample = 0.000500  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[20 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01409654 * 32000; errs = 0.425% * 32000; time = 54.9873s; samplesPerSecond = 582.0
Finished Epoch[20 of 40]: [Training] ce = 0.01412785 * 60000; errs = 0.428% * 60000; totalSamplesSeen = 1200000; learningRatePerSample = 0.00050000002; epochTime=103.815s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.20'

Starting Epoch 21: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[21 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01217201 * 32000; errs = 0.381% * 32000; time = 55.9109s; samplesPerSecond = 572.3
Finished Epoch[21 of 40]: [Training] ce = 0.01230640 * 60000; errs = 0.378% * 60000; totalSamplesSeen = 1260000; learningRatePerSample = 9.9999997e-05; epochTime=104.624s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.21'

Starting Epoch 22: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[22 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01006740 * 32000; errs = 0.291% * 32000; time = 55.3965s; samplesPerSecond = 577.7
Finished Epoch[22 of 40]: [Training] ce = 0.01143802 * 60000; errs = 0.348% * 60000; totalSamplesSeen = 1320000; learningRatePerSample = 9.9999997e-05; epochTime=104.341s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.22'

Starting Epoch 23: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[23 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01087962 * 32000; errs = 0.306% * 32000; time = 55.2253s; samplesPerSecond = 579.4
Finished Epoch[23 of 40]: [Training] ce = 0.01090899 * 60000; errs = 0.307% * 60000; totalSamplesSeen = 1380000; learningRatePerSample = 9.9999997e-05; epochTime=103.882s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.23'

Starting Epoch 24: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[24 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01162685 * 32000; errs = 0.325% * 32000; time = 56.5253s; samplesPerSecond = 566.1
Finished Epoch[24 of 40]: [Training] ce = 0.01139290 * 60000; errs = 0.323% * 60000; totalSamplesSeen = 1440000; learningRatePerSample = 9.9999997e-05; epochTime=104.94s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.24'

Starting Epoch 25: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[25 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01203397 * 32000; errs = 0.366% * 32000; time = 55.9531s; samplesPerSecond = 571.9
Finished Epoch[25 of 40]: [Training] ce = 0.01145476 * 60000; errs = 0.345% * 60000; totalSamplesSeen = 1500000; learningRatePerSample = 9.9999997e-05; epochTime=104.662s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.25'

Starting Epoch 26: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[26 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.00989036 * 32000; errs = 0.312% * 32000; time = 55.6969s; samplesPerSecond = 574.5
Finished Epoch[26 of 40]: [Training] ce = 0.01023238 * 60000; errs = 0.302% * 60000; totalSamplesSeen = 1560000; learningRatePerSample = 9.9999997e-05; epochTime=104.049s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.26'

Starting Epoch 27: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[27 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01029657 * 32000; errs = 0.300% * 32000; time = 55.0790s; samplesPerSecond = 581.0
Finished Epoch[27 of 40]: [Training] ce = 0.01009775 * 60000; errs = 0.290% * 60000; totalSamplesSeen = 1620000; learningRatePerSample = 9.9999997e-05; epochTime=103.537s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.27'

Starting Epoch 28: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[28 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.00963145 * 32000; errs = 0.253% * 32000; time = 55.4263s; samplesPerSecond = 577.3
Finished Epoch[28 of 40]: [Training] ce = 0.01071343 * 60000; errs = 0.302% * 60000; totalSamplesSeen = 1680000; learningRatePerSample = 9.9999997e-05; epochTime=104.649s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.28'

Starting Epoch 29: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[29 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01100651 * 32000; errs = 0.303% * 32000; time = 54.7451s; samplesPerSecond = 584.5
Finished Epoch[29 of 40]: [Training] ce = 0.01021849 * 60000; errs = 0.282% * 60000; totalSamplesSeen = 1740000; learningRatePerSample = 9.9999997e-05; epochTime=102.602s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.29'

Starting Epoch 30: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[30 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.00945469 * 32000; errs = 0.269% * 32000; time = 56.0213s; samplesPerSecond = 571.2
Finished Epoch[30 of 40]: [Training] ce = 0.00977264 * 60000; errs = 0.263% * 60000; totalSamplesSeen = 1800000; learningRatePerSample = 9.9999997e-05; epochTime=104.58s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.30'

Starting Epoch 31: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[31 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01070147 * 32000; errs = 0.303% * 32000; time = 55.8369s; samplesPerSecond = 573.1
Finished Epoch[31 of 40]: [Training] ce = 0.01076873 * 60000; errs = 0.325% * 60000; totalSamplesSeen = 1860000; learningRatePerSample = 9.9999997e-05; epochTime=104.382s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.31'

Starting Epoch 32: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[32 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01014149 * 32000; errs = 0.287% * 32000; time = 55.8290s; samplesPerSecond = 573.2
Finished Epoch[32 of 40]: [Training] ce = 0.00992371 * 60000; errs = 0.288% * 60000; totalSamplesSeen = 1920000; learningRatePerSample = 9.9999997e-05; epochTime=104.357s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.32'

Starting Epoch 33: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[33 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01046996 * 32000; errs = 0.300% * 32000; time = 55.4601s; samplesPerSecond = 577.0
Finished Epoch[33 of 40]: [Training] ce = 0.00958816 * 60000; errs = 0.283% * 60000; totalSamplesSeen = 1980000; learningRatePerSample = 9.9999997e-05; epochTime=103.684s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.33'

Starting Epoch 34: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[34 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.00970132 * 32000; errs = 0.272% * 32000; time = 54.6989s; samplesPerSecond = 585.0
Finished Epoch[34 of 40]: [Training] ce = 0.00992900 * 60000; errs = 0.295% * 60000; totalSamplesSeen = 2040000; learningRatePerSample = 9.9999997e-05; epochTime=103.341s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.34'

Starting Epoch 35: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[35 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.00961861 * 32000; errs = 0.297% * 32000; time = 55.7148s; samplesPerSecond = 574.4
Finished Epoch[35 of 40]: [Training] ce = 0.00956332 * 60000; errs = 0.280% * 60000; totalSamplesSeen = 2100000; learningRatePerSample = 9.9999997e-05; epochTime=104.488s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.35'

Starting Epoch 36: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[36 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.01028620 * 32000; errs = 0.297% * 32000; time = 55.7474s; samplesPerSecond = 574.0
Finished Epoch[36 of 40]: [Training] ce = 0.00928544 * 60000; errs = 0.283% * 60000; totalSamplesSeen = 2160000; learningRatePerSample = 9.9999997e-05; epochTime=103.866s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.36'

Starting Epoch 37: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[37 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.00913996 * 32000; errs = 0.216% * 32000; time = 56.5233s; samplesPerSecond = 566.1
Finished Epoch[37 of 40]: [Training] ce = 0.00919140 * 60000; errs = 0.237% * 60000; totalSamplesSeen = 2220000; learningRatePerSample = 9.9999997e-05; epochTime=104.943s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.37'

Starting Epoch 38: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[38 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.00978236 * 32000; errs = 0.284% * 32000; time = 56.0843s; samplesPerSecond = 570.6
Finished Epoch[38 of 40]: [Training] ce = 0.00999312 * 60000; errs = 0.280% * 60000; totalSamplesSeen = 2280000; learningRatePerSample = 9.9999997e-05; epochTime=105.41s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.38'

Starting Epoch 39: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[39 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.00952684 * 32000; errs = 0.294% * 32000; time = 54.9446s; samplesPerSecond = 582.4
Finished Epoch[39 of 40]: [Training] ce = 0.00889960 * 60000; errs = 0.262% * 60000; totalSamplesSeen = 2340000; learningRatePerSample = 9.9999997e-05; epochTime=103.793s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST.39'

Starting Epoch 40: learning rate per sample = 0.000100  effective momentum = 0.939413  momentum as time constant = 1024.0 samples

Starting minibatch loop.
 Epoch[40 of 40]-Minibatch[   1- 500, 53.33%]: ce = 0.00858327 * 32000; errs = 0.291% * 32000; time = 56.0703s; samplesPerSecond = 570.7
Finished Epoch[40 of 40]: [Training] ce = 0.00926387 * 60000; errs = 0.282% * 60000; totalSamplesSeen = 2400000; learningRatePerSample = 9.9999997e-05; epochTime=105.047s
SGD: Saving checkpoint model './Output/Models/ConvNet_MNIST'

Action "train" complete.


##############################################################################
#                                                                            #
# testNetwork command (test action)                                          #
#                                                                            #
##############################################################################



Post-processing network...

3 roots:
	ce = CrossEntropyWithSoftmax()
	errs = ClassificationError()
	ol = Plus()

Validating network. 33 nodes to process in pass 1.

Validating --> labels = InputValue() :  -> [10 x *1]
Validating --> model.arrayOfFunctions[12].W = LearnableParameter() :  -> [10 x 96]
Validating --> model.arrayOfFunctions[9].arrayOfFunctions[0].W = LearnableParameter() :  -> [96 x 3 x 3 x 64]
Validating --> model.arrayOfFunctions[7].W = LearnableParameter() :  -> [3 x 3 x 48 x 64]
Validating --> model.arrayOfFunctions[4].W = LearnableParameter() :  -> [3 x 3 x 32 x 48]
Validating --> model.arrayOfFunctions[1].W = LearnableParameter() :  -> [5 x 5 x 1 x 32]
Validating --> ol.x._._.x._.x.x._.x.x._.x.ElementTimesArgs[0] = LearnableParameter() :  -> [1 x 1]
Validating --> features = InputValue() :  -> [28 x 28 x 1 x *1]
Validating --> ol.x._._.x._.x.x._.x.x._.x = ElementTimes (ol.x._._.x._.x.x._.x.x._.x.ElementTimesArgs[0], features) : [1 x 1], [28 x 28 x 1 x *1] -> [28 x 28 x 1 x *1]
Validating --> ol.x._._.x._.x.x._.x.x._.c = Convolution (model.arrayOfFunctions[1].W, ol.x._._.x._.x.x._.x.x._.x) : [5 x 5 x 1 x 32], [28 x 28 x 1 x *1] -> [28 x 28 x 32 x *1]
Validating --> model.arrayOfFunctions[1].b = LearnableParameter() :  -> [1 x 1 x 32]
Validating --> ol.x._._.x._.x.x._.x.x._.res.x = Plus (ol.x._._.x._.x.x._.x.x._.c, model.arrayOfFunctions[1].b) : [28 x 28 x 32 x *1], [1 x 1 x 32] -> [28 x 28 x 32 x *1]
Validating --> ol.x._._.x._.x.x._.x.x = RectifiedLinear (ol.x._._.x._.x.x._.x.x._.res.x) : [28 x 28 x 32 x *1] -> [28 x 28 x 32 x *1]
Validating --> ol.x._._.x._.x.x._.x = Pooling (ol.x._._.x._.x.x._.x.x) : [28 x 28 x 32 x *1] -> [13 x 13 x 32 x *1]
Validating --> ol.x._._.x._.x.x._.c = Convolution (model.arrayOfFunctions[4].W, ol.x._._.x._.x.x._.x) : [3 x 3 x 32 x 48], [13 x 13 x 32 x *1] -> [11 x 11 x 48 x *1]
Validating --> model.arrayOfFunctions[4].b = LearnableParameter() :  -> [1 x 1 x 48]
Validating --> ol.x._._.x._.x.x._.res.x = Plus (ol.x._._.x._.x.x._.c, model.arrayOfFunctions[4].b) : [11 x 11 x 48 x *1], [1 x 1 x 48] -> [11 x 11 x 48 x *1]
Validating --> ol.x._._.x._.x.x = RectifiedLinear (ol.x._._.x._.x.x._.res.x) : [11 x 11 x 48 x *1] -> [11 x 11 x 48 x *1]
Validating --> ol.x._._.x._.x = Pooling (ol.x._._.x._.x.x) : [11 x 11 x 48 x *1] -> [5 x 5 x 48 x *1]
Validating --> ol.x._._.x._.c = Convolution (model.arrayOfFunctions[7].W, ol.x._._.x._.x) : [3 x 3 x 48 x 64], [5 x 5 x 48 x *1] -> [3 x 3 x 64 x *1]
Validating --> model.arrayOfFunctions[7].b = LearnableParameter() :  -> [1 x 1 x 64]
Validating --> ol.x._._.x._.res.x = Plus (ol.x._._.x._.c, model.arrayOfFunctions[7].b) : [3 x 3 x 64 x *1], [1 x 1 x 64] -> [3 x 3 x 64 x *1]
Validating --> _ol.x._._.x = RectifiedLinear (ol.x._._.x._.res.x) : [3 x 3 x 64 x *1] -> [3 x 3 x 64 x *1]
Validating --> ol.x._._.x.PlusArgs[0] = Times (model.arrayOfFunctions[9].arrayOfFunctions[0].W, _ol.x._._.x) : [96 x 3 x 3 x 64], [3 x 3 x 64 x *1] -> [96 x *1]
Validating --> model.arrayOfFunctions[9].arrayOfFunctions[0].b = LearnableParameter() :  -> [96]
Validating --> ol.x._._.x = Plus (ol.x._._.x.PlusArgs[0], model.arrayOfFunctions[9].arrayOfFunctions[0].b) : [96 x *1], [96] -> [96 x *1]
Validating --> ol.x._ = Dropout (ol.x._._.x) : [96 x *1] -> [96 x *1]
Validating --> ol.x = RectifiedLinear (ol.x._) : [96 x *1] -> [96 x *1]
Validating --> ol.PlusArgs[0] = Times (model.arrayOfFunctions[12].W, ol.x) : [10 x 96], [96 x *1] -> [10 x *1]
Validating --> model.arrayOfFunctions[12].b = LearnableParameter() :  -> [10]
Validating --> ol = Plus (ol.PlusArgs[0], model.arrayOfFunctions[12].b) : [10 x *1], [10] -> [10 x *1]
Validating --> ce = CrossEntropyWithSoftmax (labels, ol) : [10 x *1], [10 x *1] -> [1]
Validating --> errs = ClassificationError (labels, ol) : [10 x *1], [10 x *1] -> [1]

Validating network. 20 nodes to process in pass 2.


Validating network, final pass.

ol.x._._.x._.x.x._.x.x._.c: using GEMM convolution engine for geometry: Input: 28 x 28 x 1, Output: 28 x 28 x 32, Kernel: 5 x 5 x 1, Map: 32, Stride: 1 x 1 x 1, Sharing: (1, 1, 1), AutoPad: (1, 1, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
ol.x._._.x._.x.x._.x: using GEMM convolution engine for geometry: Input: 28 x 28 x 32, Output: 13 x 13 x 32, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1, 1, 1), AutoPad: (0, 0, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
ol.x._._.x._.x.x._.c: using GEMM convolution engine for geometry: Input: 13 x 13 x 32, Output: 11 x 11 x 48, Kernel: 3 x 3 x 32, Map: 48, Stride: 1 x 1 x 32, Sharing: (1, 1, 1), AutoPad: (0, 0, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
ol.x._._.x._.x: using GEMM convolution engine for geometry: Input: 11 x 11 x 48, Output: 5 x 5 x 48, Kernel: 3 x 3 x 1, Map: 1, Stride: 2 x 2 x 1, Sharing: (1, 1, 1), AutoPad: (0, 0, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.
ol.x._._.x._.c: using GEMM convolution engine for geometry: Input: 5 x 5 x 48, Output: 3 x 3 x 64, Kernel: 3 x 3 x 48, Map: 64, Stride: 1 x 1 x 48, Sharing: (1, 1, 1), AutoPad: (0, 0, 0), LowerPad: 0 x 0 x 0, UpperPad: 0 x 0 x 0.



Post-processing network complete.

evalNodeNames are not specified, using all the default evalnodes and training criterion nodes.


Allocating matrices for forward and/or backward propagation.

Memory Sharing: Out of 33 matrices, 0 are shared as 0, and 33 are not shared.


Minibatch[1-10]: errs = 0.560% * 10000; ce = 0.01894802 * 10000
Final Results: Minibatch[1-10]: errs = 0.560% * 10000; ce = 0.01894802 * 10000; perplexity = 1.01912867

Action "test" complete.

COMPLETED.
